{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "import jieba\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import joblib\n",
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load data and preprocess\n",
    "\n",
    "We are using [ASAP](https://github.com/Meituan-Dianping/asap) dataset authored by Bu et. al. ASAP is a Chinese restaurant review dataset collected from Dianping App. Reviews are written in Chinese and each review is annotated with a star rating from 1 to 5 and 18 different aspects along with the sentiment. \n",
    "\n",
    "\n",
    "Each aspect category for example Location#Transportation is is labeled as 1(Positive), 0(Neutral), −1(Negative), −2(Not-Mentioned). The data is conveniently splited into train, dev, test dataset already.\n",
    "\n",
    "[jieba](https://github.com/fxsjy/jieba) is used \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.304 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    words = jieba.cut(text)\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "def convert_sentiment(score):\n",
    "    if score == -2:\n",
    "        return \"not_mentioned\"\n",
    "    elif score == -1:\n",
    "        return \"negative\"\n",
    "    elif score == 0:\n",
    "        return \"neutral\"\n",
    "    else:  # score == 1\n",
    "        return \"positive\"\n",
    "\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Define aspects, e.g. Food#Appearance, Service#Price, etc.\n",
    "    aspect_columns = [col for col in df.columns if col not in [\"id\", \"review\", \"star\"]]\n",
    "    y = df[aspect_columns]\n",
    "\n",
    "    # Convert sentiment scores to categorical labels\n",
    "    y = df[aspect_columns].astype(\"object\")\n",
    "    for col in y.columns:\n",
    "        y.loc[:, col] = y[col].apply(convert_sentiment)\n",
    "\n",
    "    # Data preprocessing\n",
    "    df[\"processed_review\"] = df[\"review\"].apply(preprocess_text)\n",
    "\n",
    "    return df[\"processed_review\"], y, aspect_columns\n",
    "\n",
    "\n",
    "train_path = \"../data/train.csv\"\n",
    "dev_path = \"../data/dev.csv\"\n",
    "test_path = \"../data/test.csv\"\n",
    "\n",
    "X_train, y_train, aspect_columns = load_and_preprocess_data(train_path)\n",
    "X_dev, y_dev, _ = load_and_preprocess_data(dev_path)\n",
    "X_test, y_test, _ = load_and_preprocess_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (36850,), (36850, 18)\n",
      "Dev shape: (4940,), (4940, 18)\n",
      "Test shape: (4940,), (4940, 18)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_review</th>\n",
       "      <th>Location#Transportation</th>\n",
       "      <th>Location#Downtown</th>\n",
       "      <th>Location#Easy_to_find</th>\n",
       "      <th>Service#Queue</th>\n",
       "      <th>Service#Hospitality</th>\n",
       "      <th>Service#Parking</th>\n",
       "      <th>Service#Timely</th>\n",
       "      <th>Price#Level</th>\n",
       "      <th>Price#Cost_effective</th>\n",
       "      <th>Price#Discount</th>\n",
       "      <th>Ambience#Decoration</th>\n",
       "      <th>Ambience#Noise</th>\n",
       "      <th>Ambience#Space</th>\n",
       "      <th>Ambience#Sanitary</th>\n",
       "      <th>Food#Portion</th>\n",
       "      <th>Food#Taste</th>\n",
       "      <th>Food#Appearance</th>\n",
       "      <th>Food#Recommend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>状元 楼 饭店 第一次 去 ， 因为 地理位置 优越 ： 在 宁波市 和 义 大道 高 、 ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>positive</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>positive</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>positive</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>我 最 爱 他们 家 的 猪手 ， 麻辣 鸡爪 ， 肉片 口磨 ， 道 道菜 都 是 家常菜...</td>\n",
       "      <td>positive</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>positive</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>positive</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>positive</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>我 是 比较 喜欢 荣 新馆 的 ， 因为 材料 新鲜 ， 服务 又 好 ， 价格 适中 ，...</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>positive</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>neutral</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>not_mentioned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.8 秒杀 的 多嘴 肉蟹 煲 ， 第一天 开业 就 去 了 ， 大众 点评 很 给 力 ...</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>neutral</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>positive</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>喜欢 KOI 好多年 了 ， 但是 看着 它 的 价格 在 一路 飙涨 ， 真心 是 有点 ...</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>positive</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>positive</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    processed_review Location#Transportation  \\\n",
       "0  状元 楼 饭店 第一次 去 ， 因为 地理位置 优越 ： 在 宁波市 和 义 大道 高 、 ...                positive   \n",
       "1  我 最 爱 他们 家 的 猪手 ， 麻辣 鸡爪 ， 肉片 口磨 ， 道 道菜 都 是 家常菜...                positive   \n",
       "2  我 是 比较 喜欢 荣 新馆 的 ， 因为 材料 新鲜 ， 服务 又 好 ， 价格 适中 ，...           not_mentioned   \n",
       "3  8.8 秒杀 的 多嘴 肉蟹 煲 ， 第一天 开业 就 去 了 ， 大众 点评 很 给 力 ...           not_mentioned   \n",
       "4  喜欢 KOI 好多年 了 ， 但是 看着 它 的 价格 在 一路 飙涨 ， 真心 是 有点 ...           not_mentioned   \n",
       "\n",
       "  Location#Downtown Location#Easy_to_find  Service#Queue Service#Hospitality  \\\n",
       "0          positive              positive  not_mentioned            positive   \n",
       "1     not_mentioned         not_mentioned  not_mentioned            positive   \n",
       "2     not_mentioned         not_mentioned  not_mentioned            positive   \n",
       "3     not_mentioned         not_mentioned       negative            positive   \n",
       "4          positive              negative  not_mentioned       not_mentioned   \n",
       "\n",
       "  Service#Parking Service#Timely    Price#Level Price#Cost_effective  \\\n",
       "0   not_mentioned  not_mentioned  not_mentioned        not_mentioned   \n",
       "1   not_mentioned  not_mentioned  not_mentioned        not_mentioned   \n",
       "2   not_mentioned  not_mentioned        neutral        not_mentioned   \n",
       "3   not_mentioned  not_mentioned        neutral        not_mentioned   \n",
       "4   not_mentioned  not_mentioned       positive        not_mentioned   \n",
       "\n",
       "  Price#Discount Ambience#Decoration Ambience#Noise Ambience#Space  \\\n",
       "0  not_mentioned            positive  not_mentioned  not_mentioned   \n",
       "1  not_mentioned       not_mentioned  not_mentioned  not_mentioned   \n",
       "2  not_mentioned       not_mentioned  not_mentioned  not_mentioned   \n",
       "3       positive       not_mentioned  not_mentioned  not_mentioned   \n",
       "4       positive       not_mentioned  not_mentioned  not_mentioned   \n",
       "\n",
       "  Ambience#Sanitary   Food#Portion Food#Taste Food#Appearance Food#Recommend  \n",
       "0     not_mentioned  not_mentioned   positive   not_mentioned  not_mentioned  \n",
       "1          positive  not_mentioned   positive   not_mentioned  not_mentioned  \n",
       "2     not_mentioned  not_mentioned    neutral        positive  not_mentioned  \n",
       "3     not_mentioned       positive   positive   not_mentioned  not_mentioned  \n",
       "4     not_mentioned       positive   positive   not_mentioned  not_mentioned  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Train shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Dev shape: {X_dev.shape}, {y_dev.shape}\")\n",
    "print(f\"Test shape: {X_test.shape}, {y_test.shape}\\n\")\n",
    "\n",
    "pd.concat([X_train, y_train], axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subdirectory for EDA plots\n",
    "OUTPUT_DIR = \"eda_plots\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Bar plot colors for sentiment categories\n",
    "SENTIMENTS = [\"not_mentioned\", \"negative\", \"neutral\", \"positive\"]\n",
    "SENTIMENT_COLORS = {\n",
    "    \"not_mentioned\": \"#808080\",  # Gray\n",
    "    \"negative\": \"#FF0000\",  # Red\n",
    "    \"neutral\": \"#1F77B4\",  # Blue\n",
    "    \"positive\": \"#2CA02C\",  # Green\n",
    "}\n",
    "\n",
    "\n",
    "def plot_aspect_mention_frequency(y, dataset_name):\n",
    "    \"\"\"Plot the frequency of aspect mentions in the dataset.\"\"\"\n",
    "    mention_freq = (y != \"not_mentioned\").mean()\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.barplot(x=mention_freq.index, y=mention_freq.values)\n",
    "    plt.title(f\"Aspect Mention Frequency in {dataset_name} Dataset\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.ylabel(\"Proportion of Reviews Mentioning Aspect\")\n",
    "    annotate_bars(ax)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        os.path.join(OUTPUT_DIR, f\"aspect_mention_frequency_{dataset_name}.png\")\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_sentiment_distribution(y, dataset_name):\n",
    "    \"\"\"Plot the sentiment distribution for each aspect in the dataset.\"\"\"\n",
    "    for aspect in y.columns:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        ax = sns.countplot(\n",
    "            data=y,\n",
    "            x=aspect,\n",
    "            order=SENTIMENTS,\n",
    "            hue=aspect,\n",
    "            palette=SENTIMENT_COLORS,\n",
    "            legend=False,\n",
    "        )\n",
    "        plt.title(f\"Sentiment Distribution for {aspect} in {dataset_name} Dataset\")\n",
    "        plt.xlabel(\"Sentiment\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        annotate_bars(ax)\n",
    "        plt.savefig(\n",
    "            os.path.join(\n",
    "                OUTPUT_DIR, f\"sentiment_distribution_{aspect}_{dataset_name}.png\"\n",
    "            )\n",
    "        )\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def annotate_bars(ax):\n",
    "    \"\"\"Annotate bars with their heights, formatting based on value range.\"\"\"\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        if height <= 0:\n",
    "            return\n",
    "        if 0 < height < 1:\n",
    "            annotation_text = f\"{height:.2f}\"\n",
    "        else:\n",
    "            # Format as whole number for other values\n",
    "            annotation_text = f\"{int(height)}\"\n",
    "\n",
    "        ax.annotate(\n",
    "            annotation_text,\n",
    "            (p.get_x() + p.get_width() / 2.0, height),\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            xytext=(0, 5),\n",
    "            textcoords=\"offset points\",\n",
    "        )\n",
    "\n",
    "\n",
    "def plot_aspect_mention_distribution(y, dataset_name):\n",
    "    \"\"\"Plot the distribution of the number of aspects mentioned per review.\"\"\"\n",
    "    num_aspects_mentioned_per_review = (y != \"not_mentioned\").sum(axis=1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = sns.histplot(\n",
    "        num_aspects_mentioned_per_review, bins=range(0, len(y.columns) + 1), kde=False\n",
    "    )\n",
    "    plt.title(\n",
    "        f\"Distribution of Number of Aspects Mentioned per Review in {dataset_name} Dataset\"\n",
    "    )\n",
    "    plt.xlabel(\"Number of Aspects Mentioned\")\n",
    "    plt.ylabel(\"Number of Reviews\")\n",
    "    annotate_bars(ax)\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, f\"num_aspects_mentioned_{dataset_name}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def summarize_dataset(y, dataset_name):\n",
    "    \"\"\"Print summary statistics for the dataset.\"\"\"\n",
    "    print(f\"{dataset_name.capitalize()}\")\n",
    "    print(f\"Total reviews: {len(y)}\")\n",
    "\n",
    "    num_aspects_mentioned_per_review = (y != \"not_mentioned\").sum(axis=1)\n",
    "    avg_mentions = num_aspects_mentioned_per_review.mean()\n",
    "\n",
    "    most_mentioned_aspect = (y != \"not_mentioned\").sum().idxmax()\n",
    "    most_mentioned_aspect_count = (y != \"not_mentioned\").sum().max()\n",
    "    most_mentioned_aspect_percentage = (\n",
    "        (y[most_mentioned_aspect] != \"not_mentioned\").sum() / len(y)\n",
    "    ) * 100\n",
    "\n",
    "    least_mentioned_aspect = (y != \"not_mentioned\").sum().idxmin()\n",
    "    least_mentioned_aspect_count = (y != \"not_mentioned\").sum().min()\n",
    "    least_mentioned_aspect_percentage = (\n",
    "        (y[least_mentioned_aspect] != \"not_mentioned\").sum() / len(y)\n",
    "    ) * 100\n",
    "\n",
    "    print(f\"Average number of aspects mentioned per review: {avg_mentions:.2f}\")\n",
    "    print(\n",
    "        f\"Most frequently mentioned aspect: {most_mentioned_aspect} {most_mentioned_aspect_count} ({most_mentioned_aspect_percentage:.2f}%)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Least frequently mentioned aspect: {least_mentioned_aspect} {least_mentioned_aspect_count} ({least_mentioned_aspect_percentage:.2f}%)\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "def perform_eda(y, dataset_name):\n",
    "    plot_aspect_mention_frequency(y, dataset_name)\n",
    "    plot_sentiment_distribution(y, dataset_name)\n",
    "    plot_aspect_mention_distribution(y, dataset_name)\n",
    "    summarize_dataset(y, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "Total reviews: 36850\n",
      "Average number of aspects mentioned per review: 5.79\n",
      "Most frequently mentioned aspect: Food#Taste 34872 (94.63%)\n",
      "Least frequently mentioned aspect: Service#Parking 2476 (6.72%)\n",
      "\n",
      "Dev\n",
      "Total reviews: 4940\n",
      "Average number of aspects mentioned per review: 5.89\n",
      "Most frequently mentioned aspect: Food#Taste 4672 (94.57%)\n",
      "Least frequently mentioned aspect: Service#Parking 323 (6.54%)\n",
      "\n",
      "Test\n",
      "Total reviews: 4940\n",
      "Average number of aspects mentioned per review: 5.74\n",
      "Most frequently mentioned aspect: Food#Taste 4679 (94.72%)\n",
      "Least frequently mentioned aspect: Service#Parking 326 (6.60%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "perform_eda(y_train, \"train\")\n",
    "perform_eda(y_dev, \"dev\")\n",
    "perform_eda(y_test, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Word embedding\n",
    "\n",
    "To perform any sort of training, we need to convert raw string (chars) into vectors so that they can be computed. There are plenty of ways to do it including Bag of Words (BoW), Word2vec, GloVe, etc... \n",
    "\n",
    "We shall try them and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Bidirectional Encoder Representations from Transformers (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "model = BertModel.from_pretrained(\"bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 4307, 1039, 3517, 7649, 2421, 5018, 671, 3613, 1343, 8024, 1728, 711, 1765, 4415, 855, 5390, 831, 6632, 8038, 1762, 2123, 3797, 2356, 1469, 721, 1920, 6887, 7770, 510, 1920, 510, 677, 8024, 7027, 7481, 6163, 934, 704, 2466, 8024, 5831, 3221, 1765, 6887, 4638, 2123, 3797, 5831, 8024, 1366, 1456, 5283, 3633, 8024, 7004, 3799, 6090, 4294, 3472, 8024, 1391, 1168, 749, 2207, 3198, 952, 4638, 1456, 6887, 8024, 1728, 711, 1343, 749, 3241, 749, 8024, 1762, 1920, 1828, 5023, 749, 671, 833, 1036, 8024, 3309, 7313, 3300, 5763, 3717, 1600, 510, 3302, 1218, 1447, 6820, 680, 872, 5464, 1921, 8024, 1168, 749, 2218, 7623, 3198, 4495, 2692, 1922, 1962, 8024, 3302, 1218, 1447, 6963, 3221, 2207, 6651, 4307, 8024, 3302, 1218, 2578, 2428, 5318, 2190, 679, 2990, 6862, 8024, 3416, 3416, 6963, 3302, 1218, 1168, 855, 8024, 4157, 6983, 3717, 6820, 5447, 2552, 4638, 680, 2769, 812, 6237, 7025, 8024, 2218, 6821, 3416, 5318, 2190, 6206, 1930, 671, 1930, 8024, 4294, 1166, 3221, 2510, 3173, 3215, 510, 3825, 5326, 1290, 8020, 4692, 3302, 1218, 4277, 2798, 4761, 6887, 1399, 2099, 8021, 738, 5314, 2769, 812, 2123, 3797, 2356, 2501, 6496, 1872, 5682, 8024, 4307, 1039, 3517, 3221, 2123, 3797, 4638, 671, 2794, 4970, 1366, 8024, 3302, 1218, 1447, 4638, 5162, 6574, 3291, 860, 4385, 2769, 812, 2123, 3797, 782, 4638, 5125, 4868, 7481, 6505, 511, 6614, 671, 702, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[CLS] 状 元 楼 饭 店 第 一 次 去 ， 因 为 地 理 位 置 优 越 ： 在 宁 波 市 和 义 大 道 高 、 大 、 上 ， 里 面 装 修 中 式 ， 菜 是 地 道 的 宁 波 菜 ， 口 味 纯 正 ， 醉 泥 螺 特 棒 ， 吃 到 了 小 时 候 的 味 道 ， 因 为 去 了 晚 了 ， 在 大 堂 等 了 一 会 儿 ， 期 间 有 茶 水 喝 、 服 务 员 还 与 你 聊 天 ， 到 了 就 餐 时 生 意 太 好 ， 服 务 员 都 是 小 跑 状 ， 服 务 态 度 绝 对 不 提 速 ， 样 样 都 服 务 到 位 ， 点 酒 水 还 耐 心 的 与 我 们 解 释 ， 就 这 样 绝 对 要 夸 一 夸 ， 特 别 是 彭 新 星 、 洪 继 华 （ 看 服 务 牌 才 知 道 名 字 ） 也 给 我 们 宁 波 市 形 象 增 色 ， 状 元 楼 是 宁 波 的 一 扇 窗 口 ， 服 务 员 的 素 质 更 体 现 我 们 宁 波 人 的 精 神 面 貌 。 赞 一 个 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "out = tokenizer.encode(\n",
    "    text=X_train[0],  # 输入文本\n",
    "    # 一律补pad到max_length长度\n",
    "    padding=\"max_length\",  # 少于max_length时就padding\n",
    "    add_special_tokens=True,\n",
    "    return_tensors=None,  # None表示不指定数据类型，默认返回list\n",
    ")\n",
    "print(out)  # [101, 2769, 3221, 2765, 3300, 1997, 102]  # token ids\n",
    "print(tokenizer.decode(out))  # 我喜欢这家餐厅的食物，和服务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Ensure the model is in evaluation mode and move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BertModel.from_pretrained(\"bert-base-chinese\").to(device)\n",
    "model.eval()  # Set to evaluation mode since we're only extracting embeddings\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tqdm import tqdm  # Import tqdm for progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the model is in evaluation mode and move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BertModel.from_pretrained(\"bert-base-chinese\").to(device)\n",
    "model.eval()  # Set to evaluation mode since we're only extracting embeddings\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "\n",
    "# Step 1: Define a function to generate BERT embeddings with a progress bar\n",
    "def get_bert_embeddings(texts, batch_size=32):\n",
    "    embeddings = []\n",
    "    num_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "\n",
    "    # Use tqdm to show a progress bar for the batches\n",
    "    for i in tqdm(\n",
    "        range(0, len(texts), batch_size),\n",
    "        total=num_batches,\n",
    "        desc=\"Generating BERT embeddings\",\n",
    "    ):\n",
    "        batch_texts = texts[i : i + batch_size]\n",
    "\n",
    "        # Tokenize the batch\n",
    "        inputs = tokenizer(\n",
    "            batch_texts.tolist(),\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512,  # BERT's max input length\n",
    "            return_tensors=\"pt\",  # Return PyTorch tensors\n",
    "            add_special_tokens=True,\n",
    "        )\n",
    "\n",
    "        # Move inputs to the device (CPU/GPU)\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "        # Get BERT embeddings (no gradient computation to save memory)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            # Use the [CLS] token embedding (first token) as the review embedding\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "\n",
    "        embeddings.append(cls_embeddings)\n",
    "\n",
    "    # Concatenate all batch embeddings\n",
    "    embeddings = np.concatenate(embeddings, axis=0)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating BERT embeddings for dev data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating BERT embeddings:  12%|█▏        | 19/155 [02:47<19:26,  8.58s/it]"
     ]
    }
   ],
   "source": [
    "print(\"Generating BERT embeddings for dev data...\")\n",
    "X_dev_bert = get_bert_embeddings(X_dev, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating BERT embeddings for training data...\")\n",
    "X_train_bert = get_bert_embeddings(X_train, batch_size=32)\n",
    "\n",
    "\n",
    "print(\"Generating BERT embeddings for test data...\")\n",
    "X_test_bert = get_bert_embeddings(X_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train a classifier for each aspect using BERT embeddings\n",
    "print(\"\\nTraining classifiers with BERT embeddings...\")\n",
    "classifiers_bert = {}\n",
    "\n",
    "for aspect in aspect_columns:\n",
    "    print(f\"Training classifier for {aspect}...\")\n",
    "    if y_train[aspect].nunique() < 2:\n",
    "        print(f\"Skipping {aspect}: only one class in training data\")\n",
    "        classifiers_bert[aspect] = None\n",
    "    else:\n",
    "        # Use LinearSVC with class weights to handle imbalance\n",
    "        clf = LinearSVC(random_state=42, class_weight=\"balanced\", max_iter=1000)\n",
    "        clf.fit(X_train_bert, y_train[aspect])\n",
    "        classifiers_bert[aspect] = clf\n",
    "\n",
    "# Step 4: Evaluate on dev set\n",
    "print(\"\\nValidation Results (Dev Set) with BERT embeddings:\")\n",
    "for aspect in aspect_columns:\n",
    "    if classifiers_bert[aspect] is not None:\n",
    "        y_pred_dev = classifiers_bert[aspect].predict(X_dev_bert)\n",
    "        print(f\"\\n{aspect}:\")\n",
    "        print(classification_report(y_dev[aspect], y_pred_dev))\n",
    "    else:\n",
    "        print(f\"\\n{aspect}: No model trained (single class)\")\n",
    "\n",
    "# Step 5: Evaluate on test set\n",
    "print(\"\\nTesting Results (Test Set) with BERT embeddings:\")\n",
    "for aspect in aspect_columns:\n",
    "    if classifiers_bert[aspect] is not None:\n",
    "        y_pred_test = classifiers_bert[aspect].predict(X_test_bert)\n",
    "        print(f\"\\n{aspect}:\")\n",
    "        print(classification_report(y_test[aspect], y_pred_test))\n",
    "    else:\n",
    "        print(f\"\\n{aspect}: No model trained (single class)\")\n",
    "\n",
    "# Step 6: Save the classifiers\n",
    "print(\"\\nSaving BERT classifiers...\")\n",
    "for aspect, clf in classifiers_bert.items():\n",
    "    if clf is not None:\n",
    "        joblib.dump(clf, f\"classifier_bert_{aspect}.pkl\")\n",
    "print(\"BERT classifiers saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
