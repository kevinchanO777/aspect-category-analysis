{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "import jieba\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import joblib\n",
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load data and preprocess\n",
    "\n",
    "We are using [ASAP](https://github.com/Meituan-Dianping/asap) dataset authored by Bu et. al. ASAP is a Chinese restaurant review dataset collected from Dianping App. Reviews are written in Chinese and each review is annotated with a star rating from 1 to 5 and 18 different aspects along with the sentiment. \n",
    "\n",
    "\n",
    "Each aspect category for example Location#Transportation is is labeled as 1(Positive), 0(Neutral), −1(Negative), −2(Not-Mentioned). The data is conveniently splited into train, dev, test dataset already.\n",
    "\n",
    "[jieba](https://github.com/fxsjy/jieba) is used \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.295 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    words = jieba.cut(text)\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "def convert_sentiment(score):\n",
    "    if score == -2:\n",
    "        return \"not_mentioned\"\n",
    "    elif score == -1:\n",
    "        return \"negative\"\n",
    "    elif score == 0:\n",
    "        return \"neutral\"\n",
    "    else:  # score == 1\n",
    "        return \"positive\"\n",
    "\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Define aspects, e.g. Food#Appearance, Service#Price, etc.\n",
    "    aspect_columns = [col for col in df.columns if col not in [\"id\", \"review\", \"star\"]]\n",
    "    y = df[aspect_columns]\n",
    "\n",
    "    # Convert sentiment scores to categorical labels\n",
    "    y = df[aspect_columns].astype(\"object\")\n",
    "    for col in y.columns:\n",
    "        y.loc[:, col] = y[col].apply(convert_sentiment)\n",
    "\n",
    "    # Data preprocessing\n",
    "    df[\"processed_review\"] = df[\"review\"].apply(preprocess_text)\n",
    "\n",
    "    return df[\"processed_review\"], y, aspect_columns\n",
    "\n",
    "\n",
    "train_path = \"../data/train.csv\"\n",
    "dev_path = \"../data/dev.csv\"\n",
    "test_path = \"../data/test.csv\"\n",
    "\n",
    "X_train, y_train, aspect_columns = load_and_preprocess_data(train_path)\n",
    "X_dev, y_dev, _ = load_and_preprocess_data(dev_path)\n",
    "X_test, y_test, _ = load_and_preprocess_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (36850,), (36850, 18)\n",
      "Dev shape: (4940,), (4940, 18)\n",
      "Test shape: (4940,), (4940, 18)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_review</th>\n",
       "      <th>Location#Transportation</th>\n",
       "      <th>Location#Downtown</th>\n",
       "      <th>Location#Easy_to_find</th>\n",
       "      <th>Service#Queue</th>\n",
       "      <th>Service#Hospitality</th>\n",
       "      <th>Service#Parking</th>\n",
       "      <th>Service#Timely</th>\n",
       "      <th>Price#Level</th>\n",
       "      <th>Price#Cost_effective</th>\n",
       "      <th>Price#Discount</th>\n",
       "      <th>Ambience#Decoration</th>\n",
       "      <th>Ambience#Noise</th>\n",
       "      <th>Ambience#Space</th>\n",
       "      <th>Ambience#Sanitary</th>\n",
       "      <th>Food#Portion</th>\n",
       "      <th>Food#Taste</th>\n",
       "      <th>Food#Appearance</th>\n",
       "      <th>Food#Recommend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>状元 楼 饭店 第一次 去 ， 因为 地理位置 优越 ： 在 宁波市 和 义 大道 高 、 ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>positive</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>positive</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>positive</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>我 最 爱 他们 家 的 猪手 ， 麻辣 鸡爪 ， 肉片 口磨 ， 道 道菜 都 是 家常菜...</td>\n",
       "      <td>positive</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>positive</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>positive</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>positive</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>我 是 比较 喜欢 荣 新馆 的 ， 因为 材料 新鲜 ， 服务 又 好 ， 价格 适中 ，...</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>positive</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>neutral</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>not_mentioned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.8 秒杀 的 多嘴 肉蟹 煲 ， 第一天 开业 就 去 了 ， 大众 点评 很 给 力 ...</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>neutral</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>positive</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>喜欢 KOI 好多年 了 ， 但是 看着 它 的 价格 在 一路 飙涨 ， 真心 是 有点 ...</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>positive</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>positive</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>not_mentioned</td>\n",
       "      <td>not_mentioned</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    processed_review Location#Transportation  \\\n",
       "0  状元 楼 饭店 第一次 去 ， 因为 地理位置 优越 ： 在 宁波市 和 义 大道 高 、 ...                positive   \n",
       "1  我 最 爱 他们 家 的 猪手 ， 麻辣 鸡爪 ， 肉片 口磨 ， 道 道菜 都 是 家常菜...                positive   \n",
       "2  我 是 比较 喜欢 荣 新馆 的 ， 因为 材料 新鲜 ， 服务 又 好 ， 价格 适中 ，...           not_mentioned   \n",
       "3  8.8 秒杀 的 多嘴 肉蟹 煲 ， 第一天 开业 就 去 了 ， 大众 点评 很 给 力 ...           not_mentioned   \n",
       "4  喜欢 KOI 好多年 了 ， 但是 看着 它 的 价格 在 一路 飙涨 ， 真心 是 有点 ...           not_mentioned   \n",
       "\n",
       "  Location#Downtown Location#Easy_to_find  Service#Queue Service#Hospitality  \\\n",
       "0          positive              positive  not_mentioned            positive   \n",
       "1     not_mentioned         not_mentioned  not_mentioned            positive   \n",
       "2     not_mentioned         not_mentioned  not_mentioned            positive   \n",
       "3     not_mentioned         not_mentioned       negative            positive   \n",
       "4          positive              negative  not_mentioned       not_mentioned   \n",
       "\n",
       "  Service#Parking Service#Timely    Price#Level Price#Cost_effective  \\\n",
       "0   not_mentioned  not_mentioned  not_mentioned        not_mentioned   \n",
       "1   not_mentioned  not_mentioned  not_mentioned        not_mentioned   \n",
       "2   not_mentioned  not_mentioned        neutral        not_mentioned   \n",
       "3   not_mentioned  not_mentioned        neutral        not_mentioned   \n",
       "4   not_mentioned  not_mentioned       positive        not_mentioned   \n",
       "\n",
       "  Price#Discount Ambience#Decoration Ambience#Noise Ambience#Space  \\\n",
       "0  not_mentioned            positive  not_mentioned  not_mentioned   \n",
       "1  not_mentioned       not_mentioned  not_mentioned  not_mentioned   \n",
       "2  not_mentioned       not_mentioned  not_mentioned  not_mentioned   \n",
       "3       positive       not_mentioned  not_mentioned  not_mentioned   \n",
       "4       positive       not_mentioned  not_mentioned  not_mentioned   \n",
       "\n",
       "  Ambience#Sanitary   Food#Portion Food#Taste Food#Appearance Food#Recommend  \n",
       "0     not_mentioned  not_mentioned   positive   not_mentioned  not_mentioned  \n",
       "1          positive  not_mentioned   positive   not_mentioned  not_mentioned  \n",
       "2     not_mentioned  not_mentioned    neutral        positive  not_mentioned  \n",
       "3     not_mentioned       positive   positive   not_mentioned  not_mentioned  \n",
       "4     not_mentioned       positive   positive   not_mentioned  not_mentioned  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Train shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Dev shape: {X_dev.shape}, {y_dev.shape}\")\n",
    "print(f\"Test shape: {X_test.shape}, {y_test.shape}\\n\")\n",
    "\n",
    "pd.concat([X_train, y_train], axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subdirectory for EDA plots\n",
    "OUTPUT_DIR = \"eda_plots\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Bar plot colors for sentiment categories\n",
    "SENTIMENTS = [\"not_mentioned\", \"negative\", \"neutral\", \"positive\"]\n",
    "SENTIMENT_COLORS = {\n",
    "    \"not_mentioned\": \"#808080\",  # Gray\n",
    "    \"negative\": \"#FF0000\",  # Red\n",
    "    \"neutral\": \"#1F77B4\",  # Blue\n",
    "    \"positive\": \"#2CA02C\",  # Green\n",
    "}\n",
    "\n",
    "\n",
    "def plot_aspect_mention_frequency(y, dataset_name):\n",
    "    \"\"\"Plot the frequency of aspect mentions in the dataset.\"\"\"\n",
    "    mention_freq = (y != \"not_mentioned\").mean()\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.barplot(x=mention_freq.index, y=mention_freq.values)\n",
    "    plt.title(f\"Aspect Mention Frequency in {dataset_name} Dataset\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.ylabel(\"Proportion of Reviews Mentioning Aspect\")\n",
    "    annotate_bars(ax)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        os.path.join(OUTPUT_DIR, f\"aspect_mention_frequency_{dataset_name}.png\")\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_sentiment_distribution(y, dataset_name):\n",
    "    \"\"\"Plot the sentiment distribution for each aspect in the dataset.\"\"\"\n",
    "    for aspect in y.columns:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        ax = sns.countplot(\n",
    "            data=y,\n",
    "            x=aspect,\n",
    "            order=SENTIMENTS,\n",
    "            hue=aspect,\n",
    "            palette=SENTIMENT_COLORS,\n",
    "            legend=False,\n",
    "        )\n",
    "        plt.title(f\"Sentiment Distribution for {aspect} in {dataset_name} Dataset\")\n",
    "        plt.xlabel(\"Sentiment\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        annotate_bars(ax)\n",
    "        plt.savefig(\n",
    "            os.path.join(\n",
    "                OUTPUT_DIR, f\"sentiment_distribution_{aspect}_{dataset_name}.png\"\n",
    "            )\n",
    "        )\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def annotate_bars(ax):\n",
    "    \"\"\"Annotate bars with their heights, formatting based on value range.\"\"\"\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        if height <= 0:\n",
    "            return\n",
    "        if 0 < height < 1:\n",
    "            annotation_text = f\"{height:.2f}\"\n",
    "        else:\n",
    "            # Format as whole number for other values\n",
    "            annotation_text = f\"{int(height)}\"\n",
    "\n",
    "        ax.annotate(\n",
    "            annotation_text,\n",
    "            (p.get_x() + p.get_width() / 2.0, height),\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            xytext=(0, 5),\n",
    "            textcoords=\"offset points\",\n",
    "        )\n",
    "\n",
    "\n",
    "def plot_aspect_mention_distribution(y, dataset_name):\n",
    "    \"\"\"Plot the distribution of the number of aspects mentioned per review.\"\"\"\n",
    "    num_aspects_mentioned_per_review = (y != \"not_mentioned\").sum(axis=1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = sns.histplot(\n",
    "        num_aspects_mentioned_per_review, bins=range(0, len(y.columns) + 1), kde=False\n",
    "    )\n",
    "    plt.title(\n",
    "        f\"Distribution of Number of Aspects Mentioned per Review in {dataset_name} Dataset\"\n",
    "    )\n",
    "    plt.xlabel(\"Number of Aspects Mentioned\")\n",
    "    plt.ylabel(\"Number of Reviews\")\n",
    "    annotate_bars(ax)\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, f\"num_aspects_mentioned_{dataset_name}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def summarize_dataset(y, dataset_name):\n",
    "    \"\"\"Print summary statistics for the dataset.\"\"\"\n",
    "    print(f\"{dataset_name.capitalize()}\")\n",
    "    print(f\"Total reviews: {len(y)}\")\n",
    "\n",
    "    num_aspects_mentioned_per_review = (y != \"not_mentioned\").sum(axis=1)\n",
    "    avg_mentions = num_aspects_mentioned_per_review.mean()\n",
    "\n",
    "    most_mentioned_aspect = (y != \"not_mentioned\").sum().idxmax()\n",
    "    most_mentioned_aspect_count = (y != \"not_mentioned\").sum().max()\n",
    "    most_mentioned_aspect_percentage = (\n",
    "        (y[most_mentioned_aspect] != \"not_mentioned\").sum() / len(y)\n",
    "    ) * 100\n",
    "\n",
    "    least_mentioned_aspect = (y != \"not_mentioned\").sum().idxmin()\n",
    "    least_mentioned_aspect_count = (y != \"not_mentioned\").sum().min()\n",
    "    least_mentioned_aspect_percentage = (\n",
    "        (y[least_mentioned_aspect] != \"not_mentioned\").sum() / len(y)\n",
    "    ) * 100\n",
    "\n",
    "    print(f\"Average number of aspects mentioned per review: {avg_mentions:.2f}\")\n",
    "    print(\n",
    "        f\"Most frequently mentioned aspect: {most_mentioned_aspect} {most_mentioned_aspect_count} ({most_mentioned_aspect_percentage:.2f}%)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Least frequently mentioned aspect: {least_mentioned_aspect} {least_mentioned_aspect_count} ({least_mentioned_aspect_percentage:.2f}%)\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "def perform_eda(y, dataset_name):\n",
    "    plot_aspect_mention_frequency(y, dataset_name)\n",
    "    plot_sentiment_distribution(y, dataset_name)\n",
    "    plot_aspect_mention_distribution(y, dataset_name)\n",
    "    summarize_dataset(y, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "Total reviews: 36850\n",
      "Average number of aspects mentioned per review: 5.79\n",
      "Most frequently mentioned aspect: Food#Taste 34872 (94.63%)\n",
      "Least frequently mentioned aspect: Service#Parking 2476 (6.72%)\n",
      "\n",
      "Dev\n",
      "Total reviews: 4940\n",
      "Average number of aspects mentioned per review: 5.89\n",
      "Most frequently mentioned aspect: Food#Taste 4672 (94.57%)\n",
      "Least frequently mentioned aspect: Service#Parking 323 (6.54%)\n",
      "\n",
      "Test\n",
      "Total reviews: 4940\n",
      "Average number of aspects mentioned per review: 5.74\n",
      "Most frequently mentioned aspect: Food#Taste 4679 (94.72%)\n",
      "Least frequently mentioned aspect: Service#Parking 326 (6.60%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "perform_eda(y_train, \"train\")\n",
    "perform_eda(y_dev, \"dev\")\n",
    "perform_eda(y_test, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Word embedding\n",
    "\n",
    "To perform any sort of training, we need to convert raw string (chars) into vectors so that they can be computed. There are plenty of ways to do it including Bag of Words (BoW), Word2vec, GloVe, etc... \n",
    "\n",
    "We shall try them and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Bidirectional Encoder Representations from Transformers (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'init_empty_weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BertModel, BertTokenizer \n\u001b[32m      3\u001b[39m tokenizer = BertTokenizer.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mbert-base-chinese\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m model = \u001b[43mBertModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbert-base-chinese\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/modeling_utils.py:279\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    281\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/modeling_utils.py:4333\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4330\u001b[39m config.name_or_path = pretrained_model_name_or_path\n\u001b[32m   4332\u001b[39m \u001b[38;5;66;03m# Instantiate model.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4333\u001b[39m model_init_context = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_init_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_quantized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_is_ds_init_called\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4335\u001b[39m config = copy.deepcopy(config)  \u001b[38;5;66;03m# We do not want to modify the config inplace in from_pretrained.\u001b[39;00m\n\u001b[32m   4336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33m_attn_implementation_autoset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/modeling_utils.py:3736\u001b[39m, in \u001b[36mPreTrainedModel.get_init_context\u001b[39m\u001b[34m(cls, is_quantized, _is_ds_init_called)\u001b[39m\n\u001b[32m   3734\u001b[39m         init_contexts.append(set_quantized_state())\n\u001b[32m   3735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3736\u001b[39m     init_contexts = [no_init_weights(), \u001b[43minit_empty_weights\u001b[49m()]\n\u001b[32m   3738\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m init_contexts\n",
      "\u001b[31mNameError\u001b[39m: name 'init_empty_weights' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "model = BertModel.from_pretrained(\"bert-base-chinese\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
